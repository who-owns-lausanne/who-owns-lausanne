{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who owns Lausanne? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "In order to run this notebook you will need multiple dependecies. We assume you have a running `conda` distribution.\n",
    "\n",
    " - `numpy` needs to be in version `>=0.15`\n",
    " - The JSON processing tool `jq` is needed. Install with your OS' package manager (e.g. `brew install jq`).\n",
    " - `pip install yq geopy shapely geopandas`\n",
    " - If you want to have a look at the raw geographical data you have to intall [QGIS](https://www.qgis.org/en/site/), an open source geo information system tool.\n",
    "\n",
    "_If you are running an older version of macOS (e.g. 10.11) you might need to call `ulimit -n 1024` in the terminal before starting `jupyter`. This can avoid a bug with one of the preprocessing scripts._\n",
    "\n",
    "## Imports\n",
    "\n",
    "We start importing some core python libraries that will be used throughout the whole Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font=\"Helvetica Neue\")\n",
    "themecolors = sns.color_palette([\"#5B698C\",\"#84CFAC\", \"#BD8273\",\"#0E285C\",  \"#B2A7DB\"])\n",
    "sns.set_palette(themecolors)\n",
    "\n",
    "sns.set_style(\"whitegrid\", {\n",
    "    \"axes.edgecolor\": \"0.9\",\n",
    "    \"grid.color\": \"0.9\"\n",
    "})\n",
    "\n",
    "# Auto reload module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Suppress future warning (generated by seaborn)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Public data and owners\n",
    "\n",
    "We obtained ftp access from the Lausanne office of cadastre. The data is a collection of ESRI shapefiles, describing roads, buildings, parcels, trees, waterbodies, and others.\n",
    "Each shapefile is a collection of features, and each feature has an associated geometry (e.g. the shape of a land parcel) and associated attributes ( e.g. the commune responsible for the parcel, the parcel number).\n",
    "\n",
    "We can explore this dataset by using GIS software that supports shapefiles. We used QGIS to explore the dataset.\n",
    "We hoped to find an attribute describing the parcel owner in the parcel shapefile layer, but it wasn't there.\n",
    "We had to resort to web scraping to recover this attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Scraping owners\n",
    "\n",
    "### 1.1.2 Download XML files\n",
    "We wanted to associate each parcel in Lausanne to an owner. To do this, we divided Lausanne's surface in rectangles, and requested parcel informations for these rectangles to a service exposing the owners name.\n",
    "The code for the scraping is in [`/scraping/owners/scrape_owners_to_xml.py`](/edit/scraping/owners/scrape_owners_to_xml.py).\n",
    "\n",
    "The result of owner scraping is a set of 400 xml files, each containing parcel information for a geographical rectangle. The data are saved in the following directory: `data/raw/owners/`.\n",
    "\n",
    "For privacy reason we decided not to push any data on the online github respository.\n",
    "\n",
    "We start exploring the raw owner xml data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"data/raw/owners/\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file is named after the coordinates, in the Swiss systems, of the top-left and bottom-right points bounding the scraped rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"data/raw/owners/\" 2>/dev/null | head -3 # suppress error message by redirecting errors to null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 \"data/raw/owners/534810.4210526316_155847.0_535161.3710526315_155589.0.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 From XML to a single JSON\n",
    "We use `yq`, `xq` and `jq` programs to extract only the features we care about from the different XML files and save them as a list of objects in a single json file.\n",
    "[`scraping/owners/multiple_xml_to_single_json.sh`](/edit/scraping/owners/multiple_xml_to_single_json.sh) is a small script leveraging the expressiveness of `jq` to efficiently concatenate the XML files into a single json, while also discarding all the attributes we have no interest in.\n",
    "\n",
    "The result file of the shell program is a JSON file`/data/owners/all_owners_dirty.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all XML into one JSON file\n",
    "\n",
    "# TODO PIETRO --> 2>&1 | head -3 non permette di eseguire la cella seguente.\n",
    "\n",
    "!scraping/owners/multiple_xml_to_single_json.sh all_owners_dirty.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see errors of the form\n",
    "```\n",
    "jq: error (at <stdin>:1): Cannot iterate over null (null)\n",
    "```\n",
    "you don't have to worry. These are just errors, when `jq` encounters the end of a file. The script is still working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Remove duplicated and  clean owners JSON\n",
    "The generated `all_owners_dirty.json` JSON has duplicate entries, entries concerning other communes than Lausanne and entries with missing owners. Furthermore, JSON is not the best format to handle tabular data. The code in [`scraping/owners/owner_json_to_clean_csv.py`](/edit/scraping/owners/owner_json_to_clean_csv.py) cleans the duplicates and tranfsorms the data into `all_owners.csv` CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dirty data and write it to 'all_owners.csv' \n",
    "import scraping.owners\n",
    "scraping.owners.owner_json_to_clean_csv.main(\n",
    "    './data/owners/all_owners_dirty.json',\n",
    "    './data/owners/all_owners.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of semplicity, we can now remove the legacy file `all_owners_dirty.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm ./data/owners/all_owners_dirty.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Joining the owners data with the cadastre shapefiles\n",
    "The result of the previous preprocessing steps is a CSV file with three columns: commune number, parcel number, and the owner name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_owners = pd.read_csv('data/owners/all_owners.csv')\n",
    "all_owners.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cadastral data - data exploration\n",
    "\n",
    "We would like to add the owner name to the attributes of the parcels shapefile that we obtained from \"Office du Cadastre\". To do so, we import the `all_owners.csv` csv and the shapefile in *QGIS*, and we join this two \"tables\" by parcel number. The resulting geographical layer contains all the geographical features representing the parcels, and additionally the owner name for each parcel. \n",
    "\n",
    "We can now export this layer as a GeoJSON, making sure to use `WGS-84` as the coordinate system, and continue our exploration.\n",
    "\n",
    "The exported *geojson* file is saved at `data/owners/all_owners_parcelles.geojson`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "all_owners_parcelles_pd = geopandas.read_file('./data/owners/all_owners_parcelles.geojson')\n",
    "\n",
    "# Drop not used columns and rename columns\n",
    "all_owners_parcelles_pd.drop(['ID_GO', 'TYPE', 'EGRID', 'GID_OLD', 'NUMCOM'], axis='columns', inplace=True)\n",
    "all_owners_parcelles_pd.rename({'NO_PARC': 'parc_num', 'proprio': 'owner', 'NOM_COM':'commune'}, \n",
    "                               axis='columns', inplace=True)\n",
    "\n",
    "all_owners_parcelles_pd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represent a parcell and each parcel has a unique owner and a *polygon* object (geometry) that represent the surface area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_owners_parcelles_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Who is the biggest real estate owners?\n",
    "\n",
    "We can now quickly answer questions such as who are the 30 biggest property owners in Lausanne, by using the number of parcels owned as a measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels_per_owner = all_owners_parcelles_pd['owner'].value_counts()\n",
    "parcels_per_owner.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the biggest owners are either corporations, pension funds, or public institutions. \n",
    "\n",
    "We note that there is no a single private person in the list. This data tell us that our analysis will have to take into consideration other kind of owners than the private ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Unique owners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the non assigned values, we can see the total number of parcels owned and the unique owners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners = all_owners_parcelles_pd['owner'].dropna()\n",
    "print('Total parcels', len(owners))\n",
    "print('Unique owners', len(owners.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost 8'000 parcels in Lausanne. \n",
    "\n",
    "We are interestd to know how many people and societies own them. This number doesn't account for PPE (_propriété par étage_, single flats owned by privates). A lower bound on the owners can be estimated by discarding the PPE entries altogether:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds_owners = len(owners[~owners.str.contains('PPE ')].unique())\n",
    "print('Unique lower-bounds owners', lower_bounds_owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lower bound for the number of owners. Although, the real number is likely to be much higher since it's unprobable that most of these unique owners are also owners of a PPE share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Visualizing the distribution of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute portion of missing values\n",
    "all_owners_parcelles_pd['owner'].isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22% of the parcels don't have owner information. Indeed, many parcels reperesent roads, and as such they didn't have an owner on the site we scraped. Also we didn't scrape the values for the northern part of Lausanne, which is mostly farmland and woods.\n",
    "\n",
    "Let's try to visualize the missing values on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "!mkdir export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heatmap import missing_values\n",
    "m = missing_values(all_owners_parcelles_pd._to_geo())\n",
    "m.save('export/missing_values.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization is not very snappy or legible, but we can interpret it as follows:\n",
    "\n",
    "- Red areas are parcels for which the proprietary is not assigned, i.e `None`. The northern parts of Lausanne were not scraped, since we didn't want to overload the scraped website and since they're mostly rural areas. It is expected that they are red.\n",
    "- Zooming into central Lausanne, we see that roads have unknown owners. This is also expected.\n",
    "- For some areas blue and red overlap, yielding purple parcels. This is because the dataset is slightly dirty and some bigger parcels with no owners _contains_ smaller parcels with known owners. Therefore the colors overlap.\n",
    "\n",
    "Having asserted that the dataset is fairly sane, we can drop the features were the owner is `np.nan`, since they will be of no use to us (roads), and will make the map drawing slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(all_owners_parcelles_pd['owner'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_owners_parcelles_pd = all_owners_parcelles_pd[~all_owners_parcelles_pd['owner'].isna()]\n",
    "len(complete_owners_parcelles_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2.4 Show parcels by owner type\n",
    "\n",
    "The parcel owner format allows us to know the category of each owner. \n",
    "We will use similar categories as the statistical office of the city of Lausanne [here](https://www.lausanne.ch/officiel/statistique/quartiers/tableaux-donnees.html):\n",
    "\n",
    "- privates\n",
    "- public institutions\n",
    "- companies (corporations)\n",
    "- cooperatives\n",
    "- pension funds\n",
    "- foundations\n",
    "- PPE\n",
    "\n",
    "Societies are detected by having 'AG' or 'SA' in their name. Similary for cooperatives, foundations, and pension funds. We display a map colored by the owner category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO NOT NEED ANOTHER PANDAS DATAFRAME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def categorize(owner):\n",
    "    regex_cats = [\n",
    "            ('retraites|pension|prévoyance|prevoyance|BVK|'+\\\n",
    "             'anlagestiftung|fondation d\\'investissement|fondation de placement|'+\\\n",
    "             'vorsorge|anlage stiftung', 'pension'),\n",
    "            ('commune de lausanne|dfire|cff|domaine public|Etat de Vaud|Service du logement', 'public'),\n",
    "            (r's\\.a\\.|\\bsa\\b|\\bag\\b|société anonyme|sàrl|\\bBCV\\b|SICAV', 'société'), # BCV société ou public?\n",
    "            (r'fondation|\\bstiftung|foundation|association|fédération', 'fondation/association'),\n",
    "            (r'\\bppe\\b|copropriété|copropriete|parcelles', 'PPE'),\n",
    "            ('société coopérative|societe cooperative', 'coop'),\n",
    "            ('.*', 'private')\n",
    "    ]\n",
    "    for cat_re, category in regex_cats: \n",
    "        if re.search(cat_re, owner, flags=re.IGNORECASE):\n",
    "            return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_owners_parcelles_pd['owner_type'] = complete_owners_parcelles_pd['owner'].apply(lambda r:\n",
    "                                                                                        categorize(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_owners_parcelles_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners_categories = complete_owners_parcelles_pd[['owner']].copy()\n",
    "owners_categories['category'] = owners_categories['owner'].apply(categorize)\n",
    "owners_categories = owners_categories.drop_duplicates().set_index('owner')\n",
    "owners_categories.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually see on the map the distribution of owners by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heatmap import by_owners_category\n",
    "m = by_owners_category(complete_owners_parcelles_pd._to_geo(), owners_categories )\n",
    "m.save(\"export/by_owners_category.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rents data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Scraping\n",
    "\n",
    "In order to analyse how ownership patterns influence prices, we needed to complement the owners dataset with rent prices.\n",
    "Rent prices are generally not public, but we can scrape from real estate websites' current rent listings, and extract the prices from there.\n",
    "\n",
    "We scraped from [anibis.ch](https://www.anibis.ch/fr/default.aspx), [homegate.ch](https://www.homegate.ch/fr) and [tutti.ch](https://tutti.ch) and extracted up-to-date real estate announcements.\n",
    "\n",
    "#### 2.1.1 Download the raw rents data\n",
    "\n",
    "The scripts to download the data from the three portals are the following:\n",
    "\n",
    "- for homegate: [`scraping/homegate/scrape_homegate.py`](/edit/scraping/homegate/scrape_homegate.py), to download and parse the data. Data are saved in `data/rents` as `homegate.json`\n",
    "\n",
    "- for anibis:\n",
    "    1. [`scraping/anibis/anibis_scrape_listings.py`](/edit/scraping/anibis/anibis_scrape_listings.py) to download the index of results matching rents in lausanne\n",
    "    2. [`scraping/anibis/anibis_scrape_offers.py`](/edit/scraping/anibis/anibis_scrape_offers.py) to download each single rent offer, given a parsed index\n",
    "\n",
    "- for tutti:\n",
    "    1. [`scraping/tutti/tutti_scrape_listings.py`](/edit/scraping/tutti/tutti_scrape_listings.py) to download the index of results matching rents in lausanne\n",
    "    \n",
    " \n",
    "The raw rents data are then saved in `data/raw`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Parse the rents data\n",
    "\n",
    "\n",
    "Once downloaded we parse the data in a agreed JSON format.\n",
    "\n",
    "#### Tutti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping.tutti import tutti_parse_listings\n",
    "tutti_parse_listings.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anibis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`scraping/anibis/anibis_parse_listings.py`](/edit/scraping/anibis/anibis_parse_listings.py) to parse the listings index.\n",
    "2. [`scraping/anibis/anibis_parse_offers.py`](/edit/scraping/anibis/anibis_parse_offers.py) to parse the pages for each offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing duplicates\n",
    "\n",
    "Most rent listings are published on several websites. When merging the data sources, we first need to figure out which results are present in multiple datasets to avoid duplicate datapoints. We consider listings to be duplicates if they have the same address and the same price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jq length data/raw/rents/tutti.json\n",
    "!jq length data/raw/rents/anibis_with_streets.json\n",
    "!jq length data/raw/rents/homegate.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before cleaning and merging we have a total of ~1300 offers.\n",
    "\n",
    "The parsed files have such structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_tutti = pd.read_json(\"./data/raw/rents/tutti.json\")\n",
    "pd_tutti.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to removing duplicates, [`cleaning/merge_rent_offers.py`](/edit/cleaning/merge_rent_offers.py) clean offers without prices, without addresses, or without surface area. \n",
    "\n",
    "The merge script return a JSON file with all listings, one each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleaning as cleaning\n",
    "filenames = [\"./data/raw/rents/tutti.json\", \n",
    "             \"./data/raw/rents/anibis_with_streets.json\",\n",
    "            \"./data/raw/rents/homegate.json\"]\n",
    "dirty_rent_prices = cleaning.merge_rent_offers.main(filenames)\n",
    "len(dirty_rent_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mapping street addresses to coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_rent_prices_pd = pd.DataFrame.from_dict(dirty_rent_prices)\n",
    "dirty_rent_prices_pd.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above we can see that the address is in textual form. \n",
    "\n",
    "Also, it is clear that there will be more cleaning needed. The first address is a phone number instead of an actual address. This sanitisation is automatically provided by the next script.\n",
    "\n",
    "To perform geographical queries on addresses, we need to convert them to coordinates. To do so, we use the cadastral layer of building addresses, provided by the Cadastral office of Lausanne.\n",
    "During merging of the three datasets, the addresses were standardized to use the format used by this cadastral layer.\n",
    "\n",
    "To map an address to a coordinates couple, we iterate over all buildings in Lausanne, and check if the street name and the street number match those of our address. If there's a match, we extract the coordinates of the building from the cadastral layer. If there isn't we drop the offer (like the phone number above) and therefore perform some cleaning.\n",
    "\n",
    "We use [`cleaning/address_to_coords.py`](/edit/cleaning/address_to_coords.py) to execute the operations descripted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_prices = cleaning.address_to_coords.main(dirty_rent_prices)\n",
    "rent_prices_pd = pd.DataFrame.from_dict(rent_prices)\n",
    "# DROP not used columns\n",
    "rent_prices_pd.drop(['city', 'meuble'], axis='columns', inplace=True)\n",
    "\n",
    "# Explode from position latitude and longitude\n",
    "long_lat = rent_prices_pd['position'].apply(lambda pos: pd.Series(pos))\n",
    "long_lat.columns = ['long', 'lat']\n",
    "long_lat.head(2)\n",
    "rent_prices_pd =  pd.concat([rent_prices_pd, long_lat], axis='columns')\n",
    "#rent_prices_pd.drop('position',axis='columns', inplace=True) # TODO\n",
    "\n",
    "# Convert surface type to float\n",
    "rent_prices_pd['surface'] = rent_prices_pd['surface'].astype(float)\n",
    "\n",
    "rent_prices_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rent_prices_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualizing the rents dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take a first look at the rental data in a cleaned form.\n",
    "\n",
    "From now on, for a better spatial understanding, we will renders many of our maps using the different quartiers of Lausanne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiers_pd = geopandas.read_file('data/raw/maps/quartiers.geojson')\n",
    "quartiers_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_prices_pd['CHF/m2'] = rent_prices_pd[['price', 'surface']].apply(lambda row:\n",
    "                                                                      float(row['price'])/float(row['surface']), \n",
    "                                                                      axis='columns')\n",
    "rent_prices_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heatmap import marker_rents_with_quartiers\n",
    "m = marker_rents_with_quartiers(rent_prices_pd, quartiers_pd._to_geo() )\n",
    "m.save(\"export/marker_rents_with_quartiers.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Mapping rent datapoints to quartiers\n",
    "Each rent data-point has a pair of coordinates localizing it in space. _quartiers_ are polygons, whose perimeter is a list of coordinates. We can use the python library `shapely`, that allows us to perform geometrical queries, to find the _quartier_ for each rent offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the two data structures needed\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move to 'tools.py'\n",
    "quartiers = quartiers_pd._to_geo()\n",
    "def get_quartier(lat, long):\n",
    "    for quartier in quartiers['features']:\n",
    "        # skip because we don't have owner data for forest areas\n",
    "        if quartier['properties']['Name'] == '90 - Zones foraines':  \n",
    "            return None\n",
    "\n",
    "        \"\"\"Given longitute and latitude, return quartier in Lausanne\"\"\"\n",
    "        pos = Point(long, lat)\n",
    "        quartier_vertices = [(east, north) for east, north, z in quartier['geometry']['coordinates'][0]]\n",
    "        quartier_poly = Polygon(quartier_vertices)\n",
    "        if quartier_poly.contains(pos):\n",
    "            return quartier['properties']['Name']\n",
    "\n",
    "\n",
    "rent_prices_pd['quartier'] = \\\n",
    "    rent_prices_pd[['position']] \\\n",
    "    .apply(lambda row: get_quartier(row['position'][1], row['position'][0]), axis='columns')\n",
    "rent_prices_pd.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sanity check by changing the color of the marker depending on the found _quartier_ and displaying all of it on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heatmap import circles_rents\n",
    "m = circles_rents(rent_prices_pd, quartiers_pd._to_geo() )\n",
    "m.save(\"export/circle_rents.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good. We will now give a first statistic on rent prices. **However**, there are still fake offers (like offers for parking lots and the like) and there are still outliers in the dataset. The results are therefore not yet _real, clean means_. \n",
    "\n",
    "As a cheap mitigation we will display the median instead of the mean. Before analysing and building our mathematical model we will however clean those offers out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display median price per neighborhood\n",
    "rents_per_quartier = rent_prices_pd[['CHF/m2', 'quartier']].groupby('quartier').agg('median')\n",
    "rents_per_quartier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_prices_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heatmap import circles_prices\n",
    "m = circles_prices(rent_prices_pd)\n",
    "m.save('export/circle_prices.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Map each offer to an owner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parcel has an owner and a geometry, which is a `MultiPolygon`. A `MultiPolygon` is a list of `Polygon`s. Every `Polygon` is a list of \"linear rings\". The first linear ring defines the outer perimeter of the polygon, and the next linear rings define holes in the polygon.\n",
    "For all parcels the multipolygons are made of only 1 polygon, and every polygon only has the outer perimeter and no holes. We can use shapely polygons again to find wether an offer is within a polygon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO COMMENTS ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_owner(pos):\n",
    "    pos = Point(pos)\n",
    "    parcelle = complete_owners_parcelles_pd[complete_owners_parcelles_pd['geometry'].contains(pos)]\n",
    "    owner = np.nan\n",
    "    if parcelle['owner'].values.size > 0:\n",
    "        owner = parcelle['owner'].values[0]\n",
    "    return owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_owner(rent_prices_pd['position'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_prices_pd['owner'] = rent_prices_pd['position'].apply(lambda pos: get_owner(pos))\n",
    "rent_prices_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(rent_prices_pd['owner'].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear model describing relation of ownership and price\n",
    "\n",
    "The data is now in the form we need in order to apply our model. As our main goal is to understand the rent price composition, we will perform linear regression on the rent prices.\n",
    "\n",
    "More precisely, we will try to predict the prices of rents in each quartier based on the features:\n",
    "\n",
    " - ownership proportion of each ownership type: ($f_{public}, f_{s.a.}, $...)\n",
    " - distance from the centre of the city: $dist$\n",
    " - the mean price of rents in the _quartier_ $q$ (dependent variable): $price(q)$ \n",
    " \n",
    "The linear model is then:\n",
    "\n",
    "$$\n",
    "price(q) = \\beta_1 f_{public}(q) + \\beta_2  f_{s.a.}(q) + ~...~ + \\beta_j  f_{privates}(q)+  \\beta_k dist(q)\n",
    "$$\n",
    "\n",
    "We will apply linear regression to this model and extract the knowledge from the parameters $\\beta$. One problem could however be, that the ownership pattern itself depends on the distance or vice-versa. In that case we'll be able to check this assumption by predicting the distance from the ownership types:\n",
    "\n",
    "$$\n",
    "dist(q) = \\beta_1 f_{public}(q) + \\beta_2  f_{s.a.}(q) + ~...~ +\\beta_j  f_{privates}(q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 influence of owner type on average price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each rents have it's own owner. We can now iterate over all rents and add a columns 'owner_type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove not assigned owner\n",
    "rent_prices_dropped_pd = rent_prices_pd.dropna().copy()\n",
    "\n",
    "rent_prices_dropped_pd['owner_type'] = rent_prices_dropped_pd['owner'].apply(lambda r: categorize(r))\n",
    "rent_prices_dropped_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO PIETRO. COMMENTS and check covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import scipy\n",
    "\n",
    "covariates = rent_prices_dropped_pd[['owner_type','CHF/m2']].copy()\n",
    "covariates[\"owner_type\"] = covariates[\"owner_type\"].astype(\"category\")\n",
    "samples_per_cat = covariates[\"owner_type\"].value_counts()\n",
    "\n",
    "covariates = pd.get_dummies(covariates)\n",
    "\n",
    "# drop one indicator to avoid multiple colinearity\n",
    "covariates = covariates.drop(\"owner_type_private\", axis=\"columns\")\n",
    "\n",
    "coeffs = np.empty(covariates.shape)\n",
    "\n",
    "# columns: num of covariates minus the response (y) plus 1 for the intercept\n",
    "\n",
    "# bootstrap confidence interval for linear regression coefficients\n",
    "for i in range(covariates.shape[0]):\n",
    "    sample = covariates.values[\n",
    "        np.random.choice(\n",
    "            covariates.shape[0], size=covariates.shape[0], replace=True\n",
    "        )\n",
    "    ]\n",
    "    lm = linear_model.LinearRegression(fit_intercept=True)\n",
    "    X = sample[:, 1:]\n",
    "    y = sample[:, 0]\n",
    "    lm.fit(X, y)\n",
    "    coeffs[i, 0] = lm.intercept_\n",
    "    coeffs[i, 1:] = lm.coef_\n",
    "\n",
    "lower, upper = np.percentile(coeffs, q=(2.5, 97.5), axis=0)\n",
    "\n",
    "print(\"%-40s\\t%s\\t%s\\t%s\" % (\"feature\", \".025 qtile\", \".975 qtile\", \"n\"))\n",
    "print()\n",
    "print(\"%-40s:\\t%f\\t%f\" % (\"intercept\", lower[0], upper[0]))\n",
    "\n",
    "for typ, lower_q, upper_q in zip(covariates.columns[1:], lower[1:], upper[1:]):\n",
    "    print(\n",
    "        \"%-40s:\\t%f\\t%f\\t%d\"\n",
    "        % (typ, lower_q, upper_q, samples_per_cat[typ.split(\"_\")[-1]])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression on distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import great_circle\n",
    "\n",
    "position = 46.50766, 6.62758\n",
    "\n",
    "rent_positions = rent_prices_pd[['CHF/m2','lat','long']]\n",
    "\n",
    "rent_distances = rent_positions.apply(\n",
    "    lambda row: great_circle((row.lat, row.long), position).km, axis=1\n",
    ")\n",
    "rent_distances = rent_distances.to_frame(\"km\")\n",
    "rent_distances[\"CHF/m2\"] = rent_positions[\"CHF/m2\"]\n",
    "rent_distances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"km\", y=\"CHF/m2\", data=rent_distances);\n",
    "plt.title(\"Distance from the station vs. rent prices\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_model = scipy.stats.linregress(\n",
    "    rent_distances.km, rent_distances[\"CHF/m2\"]\n",
    ")\n",
    "\n",
    "def print_model(model):\n",
    "    for stat in [\"intercept\", \"slope\", \"stderr\", \"pvalue\"]:\n",
    "        print(stat, \": \", getattr(model, stat))\n",
    "\n",
    "        \n",
    "print_model(distance_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_model = scipy.stats.linregress(\n",
    "    rent_positions.lat ** 2, rent_positions[\"CHF/m2\"]\n",
    ")\n",
    "long_model = scipy.stats.linregress(\n",
    "    rent_positions.long ** 2, rent_positions[\"CHF/m2\"]\n",
    ")\n",
    "\n",
    "print_model(lat_model)\n",
    "print()\n",
    "print_model(long_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rents_per_quartier = rent_prices_pd[['CHF/m2','lat','long', 'surface', 'quartier', 'owner']].copy()\n",
    "\n",
    "rents_per_quartier[\"distance\"] = rent_distances.km\n",
    "rents_per_quartier = rents_per_quartier.dropna() # not anmore necessary, right?\n",
    "\n",
    "print_model(\n",
    "    scipy.stats.linregress(\n",
    "        rents_per_quartier.surface, rents_per_quartier[\"CHF/m2\"]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = rents_per_quartier[[\"distance\", \"surface\", \"CHF/m2\"]].copy()\n",
    "plot_df.columns = [\n",
    "    \"distance from the port Ouchy\", \n",
    "    \"surface of the flat\", \n",
    "    \"rent price in CHF/m^2\"\n",
    "]\n",
    "\n",
    "pair = sns.pairplot(\n",
    "    plot_df, \n",
    "    x_vars=[\"distance from the port Ouchy\", \"surface of the flat\"], \n",
    "    y_vars=[\"rent price in CHF/m^2\"],\n",
    "    height=6,\n",
    "    aspect=1,\n",
    "    kind=\"reg\"\n",
    ");\n",
    "pair.fig.suptitle(\"Rent price relationships\");\n",
    "plt.savefig(\"export/distance.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict prices for all parcelles \n",
    "\n",
    "We want now find the prices (CHF/m2) of rents for all parcels. To do that we use k-nearest-neighbor machine learning algorithm provided by *sklearn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data to be trained\n",
    "train = rent_prices_pd[['CHF/m2', 'long', 'lat']].copy()\n",
    "train.rename({'CHF/m2':'target'},axis='columns',inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_owners_parcelles_pd['long'] = all_owners_parcelles_pd['geometry'].apply(lambda poly: poly.centroid.x)\n",
    "all_owners_parcelles_pd['lat'] = all_owners_parcelles_pd['geometry'].apply(lambda poly: poly.centroid.y)\n",
    "predict = all_owners_parcelles_pd[['parc_num', 'long', 'lat']].copy()\n",
    "predict.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO link model_price_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict prices\n",
    "from machine_learning import model_price_knn\n",
    "\n",
    "predicted_chfm2, rmse, ks = model_price_knn(train, predict, np.arange(1, 50))\n",
    "\n",
    "pd.DataFrame(rmse, ks).plot(legend=False);\n",
    "plt.xlabel(\"k\");\n",
    "plt.ylabel(\"rmse\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now export the maps with all prices by parcelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_owners_parcelles_pd['CHF/m2'] = predicted_chfm2\n",
    "all_owners_parcelles_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save heatmap with prices for each parcelle\n",
    "from heatmap import parcelles_prices\n",
    "m = parcelles_prices(all_owners_parcelles_pd.dropna())\n",
    "m.save('export/parcelles_prices.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price by quartier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find out the average price for each quartier taking into account all prices of the parcelles predicted in the previous section.\n",
    "\n",
    "For a correct rendering, we want to print only the quartiers from where we scraped the date. We therefore remove the two quartiers not scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '90 - Zones foraines' and '17 - Beaulieu / Grey / Boisy from quartiers\n",
    "quartiers_to_drop = (quartiers_pd['Name'] == '17 - Beaulieu / Grey / Boisy') | \\\n",
    "                    (quartiers_pd['Name'] == '90 - Zones foraines')\n",
    "scraped_quartiers_pd = quartiers_pd[~quartiers_to_drop].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute now, the average price for each quartier considering all parcelles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each parcelle, compute quartier\n",
    "all_owners_parcelles_pd['quartier'] = \\\n",
    "    all_owners_parcelles_pd[['lat', 'long']] \\\n",
    "    .apply(lambda row: get_quartier(row['lat'], row['long']), axis='columns')\n",
    "\n",
    "# group by quartier\n",
    "price_by_quartier = all_owners_parcelles_pd[['CHF/m2', 'quartier']] \\\n",
    "                        .groupby('quartier').agg('median')\n",
    "price_by_quartier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge the `scraped_quartiers_pd` that contains the geometry of the quartiers with `price_by_quartier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge scraped quartiers with price_by_quartier\n",
    "scraped_quartiers_pd = scraped_quartiers_pd.merge(price_by_quartier, left_on='Name', right_index=True)\n",
    "scraped_quartiers_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heatmap import parcelles_prices_by_quartiers\n",
    "m = parcelles_prices_by_quartiers(scraped_quartiers_pd)\n",
    "m.save('export/parcelles_prices_by_quartiers.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot heatmap above, we can notice that quartiers close to the lake are more expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising owner type - TODO PIETRO\n",
    "\n",
    "We want to revisit the colorful owner types map by trying to find spatial clusters with a certain ownership type.\n",
    "We will approach this problem as a denoising one, and we will attribute to each parcel an owner type which is given by the category the most represented in its local neighborhood.\n",
    "We therefore find the K nearest neighbors to a parcel (itself included), and assign as type the most represented type in the neighbors.\n",
    "\n",
    "- Find a representative point for each parcel (maybe the center of mass of the parcel)\n",
    "- For each parcel find its nearest neighbors\n",
    "- compute the distribution of ownership for each neihborhood\n",
    "- assign to the parcel the ownership in the neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO explain\n",
    "EARTH_RADIUS_METERS = 6.3e6\n",
    "meters_per_easting_degree = 2*(EARTH_RADIUS_METERS*np.sin(np.pi/4))*np.pi/360\n",
    "meters_per_northing_degree = 2*EARTH_RADIUS_METERS*np.pi/360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_per_easting_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_per_northing_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_owners_parcelles_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "types = complete_owners_parcelles_pd[['parc_num', 'owner_type']].copy().set_index('parc_num').sort_index()['owner_type']\n",
    "for (idx, (parc_no, proprio, _, owner_type, x, y)) in complete_owners_parcelles_pd.iterrows():\n",
    "    distances2 = compute_distance2(polygons_df['x'] - x, polygons_df['y'] - y)\n",
    "    neigh = distances2.sort_values()[:K]\n",
    "    neighbor_parcels = pd.concat((polygons_df, neigh), axis='columns', join='inner')\n",
    "    types.loc[parc_no] = neighbor_parcels['owner_type'].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_df['owner_type'].value_counts().plot.pie();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types.value_counts().plot.pie();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance2(delta_east, delta_north):\n",
    "    \"\"\"squared distance in meters\"\"\"\n",
    "    return (delta_east*meters_per_easting_degree)**2 +\\\n",
    "        (delta_north*meters_per_northing_degree)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygons_intersection(poly):\n",
    "    def inters(serie):\n",
    "            return serie['poly'].intersection(poly).area\n",
    "    return inters\n",
    "radius = 100\n",
    "types = polygons_df[['parc_no', 'owner_type']].copy().set_index('parc_no').sort_index()['owner_type']\n",
    "for (idx, (parc_no, proprio, poly, owner_type, x, y)) in polygons_df.iterrows():\n",
    "    distances2 = compute_distance2(polygons_df['x'] - x, polygons_df['y'] - y)\n",
    "    neigh = distances2[distances2 < radius**2]\n",
    "    circle = poly.centroid.buffer(radius/meters_per_easting_degree)\n",
    "    neighbor_parcels = pd.concat((polygons_df, neigh), axis='columns', join='inner')\n",
    "    neighbor_parcels['intersection'] = neighbor_parcels.apply(polygons_intersection(circle), axis='columns')\n",
    "    intersection_per_cat = neighbor_parcels.groupby('owner_type')['intersection'].sum()\n",
    "    types.loc[parc_no] = intersection_per_cat.sort_values().index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = getMap()\n",
    "#TODO weight by area\n",
    "def style_function(feature):\n",
    "    colors = {\n",
    "        'coop': 'yellow',\n",
    "        'société' : 'red',\n",
    "        'public' : 'green',\n",
    "        'private': 'blue',\n",
    "        'PPE': 'orange',\n",
    "        'pension': 'purple',\n",
    "        'fondation/association' : 'brown'\n",
    "        \n",
    "    }\n",
    "    parc_num = feature['properties']['NO_PARC']\n",
    "    cat = types[parc_num]\n",
    "    \n",
    "    return {\n",
    "        'stroke':False,\n",
    "        'fillColor': colors[cat]\n",
    "    }\n",
    "\n",
    "folium.GeoJson(\n",
    "    geo_parcels, \n",
    "    style_function=style_function,\n",
    "    # show the owner at hover\n",
    ").add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygons_intersection(poly):\n",
    "    def inters(serie):\n",
    "            return serie['poly'].intersection(poly).area\n",
    "    return inters\n",
    "radius = 100\n",
    "diverse = polygons_df[['parc_no', 'owner_type']].copy().set_index('parc_no').sort_index()['owner_type']\n",
    "for (idx, (parc_no, proprio, poly, owner_type, x, y)) in polygons_df.iterrows():\n",
    "    distances2 = compute_distance2(polygons_df['x'] - x, polygons_df['y'] - y)\n",
    "    neigh = distances2[distances2 < radius**2]\n",
    "    circle = poly.centroid.buffer(radius/meters_per_easting_degree)\n",
    "    neighbor_parcels = pd.concat((polygons_df, neigh), axis='columns', join='inner')\n",
    "    neighbor_parcels['intersection'] = neighbor_parcels.apply(polygons_intersection(circle), axis='columns')\n",
    "    intersection_per_owner = neighbor_parcels.groupby('proprio')['intersection'].sum()\n",
    "    p = intersection_per_owner / intersection_per_cat.sum()\n",
    "    diverse.loc[parc_no] = scipy.stats.entropy(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = getMap()\n",
    "#TODO weight by area\n",
    "min_s, max_s = 1, np.quantile(diverse.values, q = .95)\n",
    "\n",
    "def style_function(feature):\n",
    "    def entropy_color(entropy):\n",
    "        rgb = cm.RdGy( (entropy - min_s) / (max_s - min_s))\n",
    "        return colors.rgb2hex(rgb)\n",
    "    \n",
    "    parc_num = feature['properties']['NO_PARC']\n",
    "    entropy = diverse[parc_num]    \n",
    "    return {\n",
    "        'stroke':False,\n",
    "        'fillColor': entropy_color(entropy),\n",
    "        'fillOpacity':0.8\n",
    "    }\n",
    "\n",
    "folium.GeoJson(\n",
    "    geo_parcels, \n",
    "    style_function=style_function,\n",
    "    # show the owner at hover\n",
    ").add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION -- TODO jonny Sunday\n",
    "\n",
    "Please find more information about further ideas, our current state of work and our story in the `README`.\n",
    "\n",
    "Because obtaining the data from all the different sources and with various methods is a huge amount of work in this project, we are not fully done with cleaning and analysing all of the datasets. As said before, the rental data still contains unwanted entries. And the data from tutti.ch has not yet been converted to the correct JSON format.\n",
    "\n",
    "However, while working on this and discussing the progress of the project we came up with a good guideline: understand how prices are determined. This is good because it lends itself well for writing a story but it also naturally yields the rather simple mathematical model from above. We are therefore convinced that we will be able to carry out the analysis to its full extent and to come up with a good web data story in the end!\n",
    "\n",
    "### Todos and future sections:\n",
    "\n",
    " - Adapt ownership categories\n",
    " - Clean fake offers and outliers\n",
    " - Linear regression on data and analysis\n",
    "     - Tune model\n",
    "     - extract parameters and CIs\n",
    "     - intra-quartier effects\n",
    "     - ...\n",
    " - Political analysis by hand\n",
    " - Check: do we answer the four RQs?\n",
    " - Graphics and maps production for web story\n",
    " \n",
    "--> The outline for the story (our end result) can be found in the `README`.\n",
    " \n",
    " - Condense information to one line about finding affordable accommodation and why it is difficult (e.g. cheap parts are far from centre/university...)\n",
    " - Write the story\n",
    " - Design web page and animations (using a static generator and `JS`)\n",
    " - Deploy site to server (github pages or own)\n",
    " \n",
    "**Milestone 3, 16.12.18**\n",
    " \n",
    " - Boil analysis down to 5 keypoints\n",
    " - Think about way of presenting this highly geographical data/problem\n",
    " - Write text for presentation and exercise!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
